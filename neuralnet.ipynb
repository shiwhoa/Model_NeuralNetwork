{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, file_to_create_df, train_test_split_size, nfeature_L1, nfeature_L2):\n",
    "        np.random.seed(1)\n",
    "\n",
    "        df_0 = pd.read_csv(file_to_create_df, header=None, index_col= None)\n",
    "\n",
    "        self.df_1 = self.preprocess(df_0)\n",
    "\n",
    "        ncols = len(self.df_1.columns)\n",
    "        nrows = len(self.df_1.index)\n",
    "        \n",
    "        self.features_arr = self.df_1.iloc[:, 0:(ncols -1)].values.reshape(nrows, ncols-1) # .values ignores header\n",
    "        self.true_target_arr = self.df_1.iloc[:, (ncols-1)].values.reshape(nrows, 1) # .values ignores header\n",
    "\n",
    "\n",
    "        nfeature_L0 = len(self.features_arr[0])\n",
    "        nsamples_L0 = len(self.true_target_arr)\n",
    "\n",
    "        if not isinstance(self.true_target_arr[0], np.ndarray):\n",
    "            nfeature_Lf = 1\n",
    "        else:\n",
    "            nfeature_Lf = len(self.true_target_arr[0])\n",
    "\n",
    "        # Split dataset into training set and testing set on the basis of test set size\n",
    "        self.features_train_arr, self.features_test_arr, self.true_target_train_arr, self.true_target_test_arr = train_test_split(self.features_arr, self.true_target_arr, test_size=train_test_split_size)\n",
    "\n",
    "        # weights go from -1 to 1\n",
    "\n",
    "        self.x_L0 = np.zeros((nsamples_L0, nfeature_L0))\n",
    "        self.x_L0 = self.features_train_arr  # 1000 samples x 10 features_L0\n",
    "        self.w_L0_L1 = 2 * np.random.random((nfeature_L0, nfeature_L1)) - 1 # 10 features_L0 x 5 features_L1\n",
    "\n",
    "        self.x_L1 = np.zeros((nsamples_L0, nfeature_L1)) # 1000 samples x 5 features_L1 (dim reduced - > like pca)\n",
    "        self.w_L1_L2 = 2 * np.random.random((nfeature_L1, nfeature_L2)) - 1 # 5 features_L1 x 3 features_L2\n",
    "        self.delta_x_L1_given_act_fn = np.zeros((nsamples_L0, nfeature_L1)) # 1000 samples x 5 features_1\n",
    "\n",
    "        self.x_L2 = np.zeros((nsamples_L0, nfeature_L2)) # 1000 samples x 3 features_L2 (dim reduced - > like pca)\n",
    "        self.w_L2_Lf = 2 * np.random.random((nfeature_L2, nfeature_Lf)) - 1 # 3 features_2 x 1 features_3 (output)\n",
    "        self.delta_x_L2_given_act_fn = np.zeros((nsamples_L0, nfeature_L2)) # 1000 samples x 3 features_L2\n",
    "\n",
    "        self.x_Lf = np.zeros((nsamples_L0, nfeature_Lf)) # 1000 samples x 1 feature_Lf (dim reduced - > like pca)\n",
    "        self.delta_x_Lf_target_given_act_fn = np.zeros((nsamples_L0, nfeature_Lf)) # 1000 samples x 1 features_Lf\n",
    "\n",
    "    def forward_pass(self, features_arr_L0, activation_fn):\n",
    "        # pass our inputs through our neural network\n",
    "        self.x_L1 = self.activation_function(features_arr_L0.dot(self.w_L0_L1), activation_fn)\n",
    "\n",
    "        self.x_L2  = self.activation_function(self.x_L1.dot(self.w_L1_L2), activation_fn)\n",
    "\n",
    "        x_Lf = self.activation_function(self.x_L2.dot(self.w_L2_Lf), activation_fn)\n",
    "\n",
    "        return x_Lf\n",
    "    \n",
    "    def backward_pass(self, x_Lf, activation_fn):\n",
    "        # pass our inputs through our neural network\n",
    "        self.compute_output_delta(x_Lf, activation_fn)\n",
    "        self.compute_hidden_layer2_delta(activation_fn)\n",
    "        self.compute_hidden_layer1_delta(activation_fn)\n",
    "\n",
    "\n",
    "    def compute_output_delta(self, x_Lf, activation_fn):\n",
    "        delta_x_Lf_target= self.true_target_train_arr - x_Lf\n",
    "\n",
    "        delta_x_Lf_target_given_act_fn = None\n",
    "\n",
    "        if activation_fn == \"sigmoid\":\n",
    "            delta_x_Lf_target_given_act_fn = delta_x_Lf_target * (self.sigmoid_derivative(self.x_Lf))\n",
    "        elif activation_fn == \"tanh\":\n",
    "            delta_x_Lf_target_given_act_fn = delta_x_Lf_target * (self.tanh_derivative(self.x_Lf))\n",
    "        elif activation_fn == \"relu\":\n",
    "            delta_x_Lf_target_given_act_fn = delta_x_Lf_target * (self.relu_derivative(self.x_Lf))\n",
    "\n",
    "        self.delta_x_Lf_target_given_act_fn = delta_x_Lf_target_given_act_fn\n",
    "\n",
    "    def compute_hidden_layer2_delta(self, activation_fn):\n",
    "        delta_x_L2 = self.delta_x_Lf_target_given_act_fn.dot(self.w_L2_Lf.T)\n",
    "        delta_x_L2_given_act_fn = None\n",
    "        if activation_fn == \"sigmoid\":\n",
    "            delta_x_L2_given_act_fn = delta_x_L2 * (self.sigmoid_derivative(self.x_L2))\n",
    "        elif activation_fn == \"tanh\":\n",
    "            delta_x_L2_given_act_fn = delta_x_L2 * (self.tanh_derivative(self.x_L2))\n",
    "        elif activation_fn == \"relu\":\n",
    "            delta_x_L2_given_act_fn = delta_x_L2 * (self.relu_derivative(self.x_L2))\n",
    "\n",
    "        self.delta_x_L2_given_act_fn = delta_x_L2_given_act_fn\n",
    "\n",
    "    def compute_hidden_layer1_delta(self, activation_fn):\n",
    "        delta_x_L1 = self.delta_x_L2_given_act_fn.dot(self.w_L1_L2.T)\n",
    "        delta_x_L1_given_act_fn = None\n",
    "        if activation_fn == \"sigmoid\":\n",
    "            delta_x_L1_given_act_fn = delta_x_L1 * (self.sigmoid_derivative(self.x_L1))\n",
    "        elif activation_fn == \"tanh\":\n",
    "            delta_x_L1_given_act_fn = delta_x_L1 * (self.tanh_derivative(self.x_L1))\n",
    "        elif activation_fn == \"relu\":\n",
    "            delta_x_L1_given_act_fn = delta_x_L1 * (self.relu_derivative(self.x_L1))\n",
    "            \n",
    "        self.delta_x_L1_given_act_fn = delta_x_L1_given_act_fn\n",
    "\n",
    "\n",
    "\n",
    "    def activation_function(self, x, activation_fn):\n",
    "        if activation_fn == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif activation_fn == \"tanh\":\n",
    "            return np.tanh(x)\n",
    "        elif activation_fn == \"relu\":\n",
    "            return np.maximum(0, x)\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "    def activation_derivative(self, x, activation=\"sigmoid\"):\n",
    "        if activation == \"sigmoid\":\n",
    "            return x * (1 - x)\n",
    "        elif activation == \"tanh\":\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "        elif activation == \"relu\":\n",
    "            return (x > 0) * 1\n",
    "    \n",
    "\n",
    "\n",
    "    def train(self, activation_fn = \"sigmoid\", max_iterations = 1000, learning_rate = 0.05):\n",
    "        for _ in range(0, max_iterations, 1):\n",
    "\n",
    "            # send forward \n",
    "            x_Lf = self.forward_pass(self.x_L0, activation_fn)\n",
    "            error = 0.5 * np.power((x_Lf - self.true_target_train_arr), 2)\n",
    "\n",
    "\n",
    "\n",
    "            # send backward\n",
    "            self.backward_pass(x_Lf, activation_fn)\n",
    "\n",
    "            w_L2_Lf_update = learning_rate * self.x_L2.T.dot(self.delta_x_Lf_target_given_act_fn) # (2, 100) dot (100 * 1)  = (2,1)\n",
    "            w_L1_L2_update = learning_rate * self.x_L1.T.dot(self.delta_x_L2_given_act_fn) # (4, 100) dot (100 * 2) = (4 * 2)\n",
    "            w_L0_L1_update = learning_rate * self.x_L0.T.dot(self.delta_x_L1_given_act_fn)\n",
    "\n",
    "            self.w_L2_Lf = self.w_L2_Lf + w_L2_Lf_update\n",
    "            self.w_L1_L2 = self.w_L1_L2 + w_L1_L2_update\n",
    "            self.w_L0_L1 = self.w_L0_L1 + w_L0_L1_update\n",
    "        \n",
    "        print(\"After \" + str(max_iterations) + \" iterations, and having learning rate as \" + str(learning_rate) + \", the total error is \" + str(np.sum(error)))\n",
    "        print(\"The final weight vectors are (starting from input to output layers)\")\n",
    "\n",
    "        print(self.w_L0_L1)\n",
    "        print(self.w_L1_L2)\n",
    "        print(self.w_L2_Lf)\n",
    "\n",
    "\n",
    "    def predict(self, activation_fn = \"sigmoid\", header = True):\n",
    "        pred_target_arr = self.forward_pass(self.features_test_arr, activation_fn)\n",
    "        error = 0.5 * np.power((pred_target_arr - self.true_target_test_arr), 2)\n",
    "        return np.sum(error)\n",
    "\n",
    "\n",
    "    def preprocess(df):\n",
    "\n",
    "        #Convert categorical attributes to numerical attributes\n",
    "        for col in df:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].astype('category').cat.codes.astype('int64')\n",
    "\n",
    "\n",
    "        arr = df.values\n",
    "\n",
    "        #Handle null or missing values\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        imputer = imputer.fit(arr)\n",
    "        arr = imputer.transform(arr)\n",
    "\n",
    "\n",
    "        #Standardization, converting mean to 0 and standard deviation to 1\n",
    "        scaler = StandardScaler().fit(arr)\n",
    "        arr = scaler.transform(arr)\n",
    "\n",
    "        df = pd.DataFrame(arr)\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "524c1bdbe95bb3b76380a5811d1583734d873139c4b342c97561ec78e6c024c6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('nlp_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
